import sys
print("Python version")
print (sys.version)

from itertools import combinations
from os.path import join, basename, splitext, isfile
from pathlib import Path
from snakemake.utils import validate
import pandas as pd
import time
import yaml
import os
import subprocess


######################################################
# config
######################################################

# write out config-file
t = time.localtime()
current_time = time.strftime("%y%m%d_%H%M", t)
config_file_name = join(config["dir_results"], "0_configs_used", "config_used_" + current_time + ".yaml")
os.makedirs(os.path.dirname(os.path.realpath(config_file_name)), exist_ok=True)
with open(config_file_name, 'w') as outfile:
    yaml.dump(config, outfile, default_flow_style=False)



######################################################
# objects
######################################################


boolean_list_TRUE = ["True", "TRUE", "true", True]
boolean_list_FALSE = ["False", "FALSE", "false", False]


run_core_preprocessing = False


if config["run_core_preprocessing"] in boolean_list_TRUE:
    run_core_preprocessing = True

print("Run following steps...")
print(" - Core preprocessing: {0}".format(run_core_preprocessing))


INPUT_RAW_FILES_for_core_preprocessing = []
SMP_IDs_for_core_preprocessing = []
if run_core_preprocessing:
    sample_sheet_for_core_preprocessing = config["sample_sheet_for_core_preprocessing"]
    sample_sheet_for_core_preprocessing_df = pd.read_csv(sample_sheet_for_core_preprocessing, sep = "\t")
    print(sample_sheet_for_core_preprocessing_df)
    INPUT_RAW_FILES_for_core_preprocessing = sample_sheet_for_core_preprocessing_df["input_files"].to_list()
    SMP_IDs_for_core_preprocessing = sample_sheet_for_core_preprocessing_df["output_sample_names"].to_list()
    sample_dictionary = dict(zip(SMP_IDs_for_core_preprocessing, INPUT_RAW_FILES_for_core_preprocessing))
    

print("INPUT_RAW_FILES_for_core_preprocessing")
print(INPUT_RAW_FILES_for_core_preprocessing)
print("SMP_IDs_for_core_preprocessing")
print(SMP_IDs_for_core_preprocessing)
print("sample_dictionary")
print(sample_dictionary)


######################################################
# file lists for rule all
######################################################



file_list_core_preprocessing = []
if config["run_core_preprocessing"]:
    tmp_list = []

    tmp_list = tmp_list + [
        ############################ info
        # create sample info
        join(config["dir_results"], config["dir_info"], "sample_info.txt"),
        ############################ stats
        # calculate the read length distribution as total count
        join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__read_length_distribution__UMI"], "0_all_samples.txt"),
        ############################ expression count matrix sequences
        # create the total count matrix with sequences as features; afterwards normalization (RPM)
        join(config["dir_results"], config["dir_expression_mat"], "raw_count__sequences.txt"),
        join(config["dir_results"], config["dir_expression_mat"], "norm_RPM_log2__sequences.txt"),
        ]

    file_list_core_preprocessing = tmp_list
    print("Core preprocessing file list for rule all:")
    print(file_list_core_preprocessing)


def get_file_list_for_rule_all(wildcards):
    file_list = []

    if config["run_core_preprocessing"]:
          file_list = file_list + file_list_core_preprocessing

    return file_list

def get_sample_input_raw_file(wildcards):
    return sample_dictionary[wildcards.sample]

######################################################
# rules
######################################################

# list all files that you want to have generated by the workflow
rule all:
    input:
        get_file_list_for_rule_all
        
        




##############################
# trimming and collapsing
##############################

# trim the full 3p-adapter and everything else after this adapter
# collapse sequences either in READ or in UMI mode; for UMI mode additional parameters are required
rule trim_and_collapse:
    input:
        ifile=get_sample_input_raw_file,
    output:
        ofile=join(config["dir_results"], config["dir_trimmed_and_collapsed"], "{sample}.fa")
    params:
        adapter_3p = config["trim_and_collapse"]["adapter_3p"],
        insert_length_min = config["trim_and_collapse"]["insert_length_min"],
        umi_mode = config["trim_and_collapse"]["umi_mode"],
        umi_length = config["trim_and_collapse"]["umi_length"],
        read_2_adapter = config["trim_and_collapse"]["read_2_adapter"],
        read_2_adapter_min_overlap = config["trim_and_collapse"]["read_2_adapter_min_overlap"]
    conda: "envs/requirements.yml"
    shell: "Rscript src/trim_and_collapse.R --input={input.ifile} --output={output.ofile} --adapter-3p={params.adapter_3p} --insert-length-min={params.insert_length_min} --use-umi-counts={params.umi_mode} --umi-length={params.umi_length} --read-2-adapter={params.read_2_adapter} --read-2-adapter-min-overlap={params.read_2_adapter_min_overlap}"

    

##############################
# statistics (sequences)
##############################

# calculate the basic fastq infos
rule determine_fastq_basic_infos:
    input:
        ifile=get_sample_input_raw_file,
    output:
        fastq_flowcell_count=join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__flowcell_stats"], "{sample}.txt"),
        fastq_index_count=join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__index_stats"], "{sample}.txt"),
        fastq_sequencer_count=join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__sequencer_stats"], "{sample}.txt"),
        fastq_total_read_count=join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__total_read_count_stats"], "{sample}.txt"),
    params:
        smpID="{sample}",
    	path_fastq_stats = join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__flowcell_stats"]),
    	path_index_stats =join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__index_stats"]),
        path_sequencer_stats =join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__sequencer_stats"]),
        path_total_read_count_stats =join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__total_read_count_stats"]),
    conda: "envs/requirements.yml"
    shell: "Rscript src/read_statistics__fastq_basic_infos.R --input={input.ifile} --input-smpID={params.smpID} --output-path-fastq-stats={params.path_fastq_stats} --output-path-index-stats={params.path_index_stats} --output-path-sequencer-stats={params.path_sequencer_stats} --output-path-total-read-count-stats={params.path_total_read_count_stats}"


# create a sample overview with flowcell and indices
rule create_sample_info:
    input:
        fastq_flowcell_count=expand(join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__flowcell_stats"], "{sample}.txt"), sample=SMP_IDs_for_core_preprocessing),
        fastq_index_count=expand(join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__index_stats"], "{sample}.txt"), sample=SMP_IDs_for_core_preprocessing),
        fastq_sequencer_count=expand(join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__sequencer_stats"], "{sample}.txt"), sample=SMP_IDs_for_core_preprocessing),
        fastq_total_read_count=expand(join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__total_read_count_stats"], "{sample}.txt"), sample=SMP_IDs_for_core_preprocessing),
        ifile_total_count_after_preprocessing = join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__read_length_distribution__UMI"], "0_all_samples.txt")
    output:
        ofile=join(config["dir_results"], config["dir_info"], "sample_info.txt"),
    params:
        output_path = join(config["dir_results"], config["dir_info"]),
        path_fastq_stats = join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__flowcell_stats"]),
        path_index_stats =join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__index_stats"]),
        path_sequencer_stats =join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__sequencer_stats"]),
        path_total_read_count_stats =join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__total_read_count_stats"]),
    conda: "envs/requirements.yml"
    shell: "Rscript src/create_sample_info.R --output-path={params.output_path} --input-path-total-read-count-before-preprocessing={params.path_total_read_count_stats} --input-file-total-count-after-preprocessing={input.ifile_total_count_after_preprocessing} --input-path-flowcell-stats={params.path_fastq_stats} --input-path-index-stats={params.path_index_stats} --input-path-sequencer-stats={params.path_sequencer_stats}"


# calculate the read length distribution as total count        
rule read_count_statistics__UMI:
    input:
        ifile = expand(join(config["dir_results"],config["dir_trimmed_and_collapsed"],"{sample}.fa"), sample=SMP_IDs_for_core_preprocessing),
    output:
        ofile = join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__read_length_distribution__UMI"], "0_all_samples.txt")
    params:
        input_path = join(config["dir_results"], config["dir_trimmed_and_collapsed"]),
        output_path = join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__read_length_distribution__UMI"]),
        count_type = config["read_stats__read_length_distribution__UMI"]["count_type"],
        is_total_count = config["read_stats__read_length_distribution__UMI"]["is_total_count"],
    conda: "envs/requirements.yml"
    shell: "Rscript src/read_statistics__read_length_distribution__total_counts.R --input-path={params.input_path} --output-path={params.output_path} --count-type={params.count_type} --total-count={params.is_total_count}"




##############################
# create count matrix (sequences)
##############################

# create the total count matrix with sequences as features
rule create_raw_count_expression_mat__sequences:
    input:
        ifile = expand(join(config["dir_results"],config["dir_trimmed_and_collapsed"],"{sample}.fa"), sample=SMP_IDs_for_core_preprocessing),
    output:
        ofile = join(config["dir_results"], config["dir_expression_mat"], "raw_count__sequences.txt"),
        ofile_seq = join(config["dir_results"], config["dir_expression_mat"], "features_all_sequences.fa"),
    params:
        input_path = join(config["dir_results"], config["dir_trimmed_and_collapsed"]),
        output_path = join(config["dir_results"], config["dir_expression_mat"]),
        feature_present_in_min_X_samples = config["expression_mat"]["feature_present_in_min_X_samples"],
        feature_with_at_least_X_count = config["expression_mat"]["feature_with_at_least_X_count"],
    conda: "envs/requirements.yml"
    shell: "Rscript src/expression_mat__deriving_features_as_sequences.R --input-path={params.input_path} --output-path={params.output_path} --feature-present-in-min-X-samples={params.feature_present_in_min_X_samples} --feature-with-at-least-X-count={params.feature_with_at_least_X_count}"

# normalization (RPM)
rule norm_RPM__seuquences:
    input:
        ifile_raw = join(config["dir_results"], config["dir_expression_mat"], "raw_count__sequences.txt"),
        ifile_total_counts = join(config["dir_results"], config["dir_read_stats"], config["dir_read_stats__read_length_distribution__UMI"], "0_all_samples.txt")
    output:
        ofile = join(config["dir_results"], config["dir_expression_mat"], "norm_RPM_log2__sequences.txt")
    params:
        input_path = join(config["dir_results"], config["dir_trimmed_and_collapsed"]),
        output_path = join(config["dir_results"], config["dir_expression_mat"]),
        feature_present_in_min_X_samples = config["expression_mat"]["feature_present_in_min_X_samples"],
    conda: "envs/requirements.yml"
    shell: "Rscript src/expression_mat__norm_RPM.R --input-file-mat={input.ifile_raw} --output-file-mat={output.ofile} --input-file-total-count={input.ifile_total_counts}"


